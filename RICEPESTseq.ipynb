{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478ff2e-b9c8-46f7-b5c5-caa3d01c61e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "242e2137-a640-476b-bdfb-b0765c474dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAF FOLDERS Folder has 1020 files\n",
      "STEM BORER Folder has 1000 files\n",
      "RICE BUG Folder has 1007 files\n",
      "Total Images in Dataset Folder: 3027\n",
      "Found 3027 files belonging to 3 classes.\n",
      "Using 1817 files for training.\n",
      "Found 3027 files belonging to 3 classes.\n",
      "Using 1210 files for validation.\n",
      "Class names: ['LEAF FOLDERS', 'RICE BUG', 'STEM BORER']\n",
      "Epoch 1/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.4263 - loss: 1.2400 - val_accuracy: 0.7314 - val_loss: 0.6773\n",
      "Epoch 2/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.7532 - loss: 0.6438 - val_accuracy: 0.8471 - val_loss: 0.4650\n",
      "Epoch 3/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.8330 - loss: 0.4705 - val_accuracy: 0.8876 - val_loss: 0.3750\n",
      "Epoch 4/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.8780 - loss: 0.3703 - val_accuracy: 0.9000 - val_loss: 0.3160\n",
      "Epoch 5/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.9100 - loss: 0.3147 - val_accuracy: 0.9149 - val_loss: 0.2765\n",
      "Epoch 6/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.9082 - loss: 0.2896 - val_accuracy: 0.9248 - val_loss: 0.2484\n",
      "Epoch 7/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9315 - loss: 0.2381 - val_accuracy: 0.9413 - val_loss: 0.2252\n",
      "Epoch 8/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.9419 - loss: 0.2260 - val_accuracy: 0.9471 - val_loss: 0.2061\n",
      "Epoch 9/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.9600 - loss: 0.1968 - val_accuracy: 0.9537 - val_loss: 0.1922\n",
      "Epoch 10/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.9624 - loss: 0.1720 - val_accuracy: 0.9579 - val_loss: 0.1785\n",
      "Epoch 11/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.9541 - loss: 0.1738 - val_accuracy: 0.9587 - val_loss: 0.1693\n",
      "Epoch 12/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.9649 - loss: 0.1596 - val_accuracy: 0.9628 - val_loss: 0.1601\n",
      "Epoch 13/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 2s/step - accuracy: 0.9688 - loss: 0.1424 - val_accuracy: 0.9628 - val_loss: 0.1514\n",
      "Epoch 14/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.9725 - loss: 0.1300 - val_accuracy: 0.9636 - val_loss: 0.1443\n",
      "Epoch 15/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 3s/step - accuracy: 0.9744 - loss: 0.1220 - val_accuracy: 0.9653 - val_loss: 0.1382\n",
      "Epoch 16/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 3s/step - accuracy: 0.9722 - loss: 0.1211 - val_accuracy: 0.9661 - val_loss: 0.1328\n",
      "Epoch 17/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 3s/step - accuracy: 0.9777 - loss: 0.1109 - val_accuracy: 0.9686 - val_loss: 0.1274\n",
      "Epoch 18/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3s/step - accuracy: 0.9724 - loss: 0.1128 - val_accuracy: 0.9678 - val_loss: 0.1235\n",
      "Epoch 19/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - accuracy: 0.9810 - loss: 0.1000 - val_accuracy: 0.9702 - val_loss: 0.1192\n",
      "Epoch 20/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 3s/step - accuracy: 0.9830 - loss: 0.0925 - val_accuracy: 0.9719 - val_loss: 0.1157\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Set up directory and parameters\n",
    "base_dir = 'Dataset/'\n",
    "img_size = 180  # Ensured to use 180 as the image size\n",
    "batch_size = 32\n",
    "image_size = (img_size, img_size)\n",
    "\n",
    "# Count total images in dataset\n",
    "count = 0\n",
    "dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "for dir in dirs:\n",
    "    files = list(os.listdir(f'{base_dir}/{dir}'))\n",
    "    print(f\"{dir} Folder has {len(files)} files\")\n",
    "    count += len(files)\n",
    "\n",
    "print(f\"Total Images in Dataset Folder: {count}\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    validation_split=0.4,\n",
    "    subset='training',\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    validation_split=0.4,\n",
    "    subset='validation',\n",
    "    seed=123,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Define the ResNet model\n",
    "resnet_model = Sequential()\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "pretrained_model = ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=(img_size, img_size, 3),\n",
    "    pooling='avg',  # Global average pooling\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add layers to the model\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(Dense(len(dirs), activation='softmax'))  # Adjust number of classes\n",
    "\n",
    "# Build the model\n",
    "resnet_model.build((None, img_size, img_size, 3))  # Set the input shape\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "history = resnet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c32f25-f8da-4d76-ba60-eaa0118f4096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "605cd744-927f-443b-a97d-e4f9c7fe1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.save('my_trained_model2.keras')  # Save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88ac56b9-9d65-45ad-ae66-55d8ec5511c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step\n",
      "The image belongs to 'RICE BUG' with a confidence level of 97.98%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('my_trained_model.keras')\n",
    "\n",
    "# Define the class names\n",
    "rice_pestnames = ['LEAF FOLDER', 'RICE BUG', 'STEM BORER']  # Ensure the order matches your training classes\n",
    "\n",
    "def classify_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    input_image = tf.keras.utils.load_img(image_path, target_size=(180, 180))  # Match your training image size\n",
    "    input_image_array = tf.keras.utils.img_to_array(input_image)\n",
    "    input_image_exp_dim = tf.expand_dims(input_image_array, 0)  # Add batch dimension\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(input_image_exp_dim)\n",
    "\n",
    "    # Get the predicted class and confidence\n",
    "    predicted_class = rice_pestnames[np.argmax(predictions)]  # Use 'predictions' variable\n",
    "    confidence = np.max(predictions) * 100  # Convert to percentage\n",
    "\n",
    "    print(f\"The image belongs to '{predicted_class}' with a confidence level of {confidence:.2f}%\")\n",
    "\n",
    "# Test with an example image\n",
    "image_path = 'rr.jpg'  # Replace with the path to your test image\n",
    "classify_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c925f34-4f80-4c1d-9e10-6fb2169dae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e64c1-9f30-45df-8535-2e84dd511417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
